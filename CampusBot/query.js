import dotenv from "dotenv";
import { GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI } from "@langchain/google-genai";
import { Pinecone } from "@pinecone-database/pinecone";
import { PineconeStore } from "@langchain/pinecone";
import promptSync from "prompt-sync";
import { ChatPromptTemplate } from "@langchain/core/prompts";

// âœ… Load environment variables
dotenv.config();
const prompt = promptSync();

// âœ… Initialize Pinecone
const pinecone = new Pinecone({ apiKey: process.env.PINECONE_API_KEY });
console.log("âœ… Connected to Pinecone");

// âœ… Connect to Pinecone index
const index = pinecone.Index(process.env.PINECONE_INDEX_NAME);

// âœ… Use Google's Gemini API for embeddings
const embeddings = new GoogleGenerativeAIEmbeddings({
  model: "text-embedding-004",
  taskType: "RETRIEVAL_DOCUMENT",
});

// âœ… Initialize Pinecone Vector Store from existing index
const vectorStore = await PineconeStore.fromExistingIndex(embeddings, {
  pineconeIndex: index,
});
console.log("âœ… Pinecone Vector Store Initialized");

// âœ… Initialize Chat Model using LangChain
const model = new ChatGoogleGenerativeAI({
  apiKey: process.env.GOOGLE_API_KEY,
  model: "gemini-2.0-flash",
});
console.log("âœ… Gemini AI Model Loaded");

// âœ… Define a Prompt Template for Chat
const promptTemplate = ChatPromptTemplate.fromMessages([
  ["system", "Answer the user's question based on the context below:\n\n{context}"],
  ["user", "{question}"],
]);

// âœ… Function to Retrieve Relevant Data from Pinecone
const retrieve = async (query) => {
  console.log(`ðŸ” Searching for: "${query}"...`);
  const retrievedDocs = await vectorStore.similaritySearch(query, 5);

  if (retrievedDocs.length === 0) {
    console.log("âŒ No relevant information found.");
    return "";
  }

  return retrievedDocs.map(doc => doc.pageContent).join("\n");
};

// âœ… Function to Generate Answer Using Gemini
const generateAnswer = async (query, context) => {
  const formattedPrompt = await promptTemplate.format({
    question: query,
    context: context,
  });

  const result = await model.invoke(formattedPrompt);
  return result.content;
};

// âœ… Interactive Chat Loop
let query = prompt("ðŸ’¬ Ask your question (type 'exit' to quit): ");
while (query.toLowerCase() !== "exit") {
  const context = await retrieve(query);
  console.log("Retrieved context is:", context)

  if (!context) {
    console.log("ðŸ¤– Answer: Sorry, I couldn't find any relevant information.");
  } else {
    const answer = await generateAnswer(query, context);
    console.log("ðŸ¤– Answer:", answer);
  }

  query = prompt("\nðŸ’¬ Ask another question (or type 'exit' to quit): ");
}

console.log("ðŸ‘‹ Chatbot session ended.");
